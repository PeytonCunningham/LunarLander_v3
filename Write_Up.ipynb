{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e46f51e0-9bad-4add-894e-77e7e3ab901c",
   "metadata": {},
   "source": [
    "# 1. Project Overview\n",
    "\n",
    "This project applies Deep Q-Network (DQN) reinforcement learning to solve the LunarLander-v3 environment from OpenAI Gymnasium.\n",
    "\n",
    "The goal is to train an agent to land a spacecraft safely between two flags using only thrust-based actions. The agent learns a policy through trial and error by interacting with the environment.\n",
    "\n",
    "### Objectives:\n",
    "- Implement a custom DQN agent\n",
    "- Train the agent to solve the environment\n",
    "- Monitor performance using reward trends and average scores\n",
    "- Generate a demo `.gif` of successful landings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace049cb-9a95-447f-b487-89dcb8a48365",
   "metadata": {},
   "source": [
    "# 2. Environment Setup\n",
    "\n",
    "The LunarLander environment simulates a spacecraft descending toward a landing pad.\n",
    "\n",
    "### Game Rules:\n",
    "- The goal is to land the spacecraft between the flags without crashing.\n",
    "- The lander has 4 actions:\n",
    "  1. Do nothing\n",
    "  2. Fire left orientation engine\n",
    "  3. Fire main engine\n",
    "  4. Fire right orientation engine\n",
    "- The agent receives:\n",
    "  - Positive reward for landing between the flags\n",
    "  - Negative reward for crashing or flying out of bounds\n",
    "  - Small rewards for controlled descent\n",
    "\n",
    "### Observation Space:\n",
    "- An 8-dimensional continuous vector:\n",
    "  - position (x, y)\n",
    "  - linear velocities (vx, vy)\n",
    "  - angle\n",
    "  - angular velocity\n",
    "  - left leg contact (bool)\n",
    "  - right leg contact (bool)\n",
    "\n",
    "### Action Space:\n",
    "- A discrete set of 4 actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c469ed71-7390-48b2-a068-52ae8a5fe531",
   "metadata": {},
   "source": [
    "# 3. Model & Approach\n",
    "\n",
    "We use a Deep Q-Network (DQN) for this problem. The DQN approximates the Q-values for each state-action pair using a feedforward neural network.\n",
    "\n",
    "### Why DQN?\n",
    "LunarLander has a continuous observation space and a discrete action space, making it a good fit for DQN. The agent learns from past experiences using experience replay and target networks to stabilize training.\n",
    "\n",
    "The model is trained using the Bellman equation with temporal difference updates.\n",
    "\n",
    "Below is the full implementation of:\n",
    "- Q-Network (`DQN Network`)\n",
    "- Experience Replay (`ReplayBuffer`)\n",
    "- Agent (`DQN Agent`)\n",
    "- Training function with comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde26bcd-80a9-4648-8fad-a52f549c36d9",
   "metadata": {},
   "source": [
    "### DQN"
   ]
  },
  {
   "cell_type": "raw",
   "id": "87fbd062-7161-444d-b441-65dd26698cb3",
   "metadata": {},
   "source": [
    "class DQNNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_layers=[128, 128]):\n",
    "        super(DQNNetwork, self).__init__()\n",
    "        layers = []\n",
    "\n",
    "        prev_size = input_dim\n",
    "        for h in hidden_layers:\n",
    "            layers.append(nn.Linear(prev_size, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_size = h\n",
    "\n",
    "        layers.append(nn.Linear(prev_size, output_dim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9168d0-93fe-4efe-ab45-6d4996c983d4",
   "metadata": {},
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "024dd735-c7ca-4a49-abe5-52e7daad52f2",
   "metadata": {},
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, batch_size):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\",\n",
    "                                     field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        batch = random.sample(self.memory, k=self.batch_size)\n",
    "        states = torch.tensor(np.vstack([e.state for e in batch]), dtype=torch.float32).to(device)\n",
    "        actions = torch.tensor(np.vstack([e.action for e in batch]), dtype=torch.int64).to(device)\n",
    "        rewards = torch.tensor(np.vstack([e.reward for e in batch]), dtype=torch.float32).to(device)\n",
    "        next_states = torch.tensor(np.vstack([e.next_state for e in batch]), dtype=torch.float32).to(device)\n",
    "        dones = torch.tensor(np.vstack([e.done for e in batch]).astype(np.uint8), dtype=torch.float32).to(device)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27225bec-425c-4785-b175-8f847e50bea1",
   "metadata": {},
   "source": [
    "### DQN Agent Class"
   ]
  },
  {
   "cell_type": "raw",
   "id": "25ee7406-6ca7-4e2a-bc9e-0140dff5dbd5",
   "metadata": {},
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, buffer_size = 100000, batch_size=64,\n",
    "                 gamma = 0.99, lr = 1e-3, tau = 1e-3, update_every = 4):\n",
    "\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.update_every = update_every\n",
    "\n",
    "        # Networks\n",
    "        self.qnetwork_local = DQNNetwork(state_size, action_size).to(device)\n",
    "        self.qnetwork_target = DQNNetwork(state_size, action_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=lr)\n",
    "\n",
    "        # Replay buffer\n",
    "        self.memory = ReplayBuffer(buffer_size, batch_size)\n",
    "        self.t_step = 0\n",
    "\n",
    "    def act(self, state, eps=0.0):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            q_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        if random.random() < eps:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "        else:\n",
    "            return np.argmax(q_values.cpu().data.numpy())\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        self.t_step = (self.t_step + 1) % self.update_every\n",
    "\n",
    "        if self.t_step == 0 and len(self.memory) >= self.batch_size:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences)\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        q_targets = rewards + (self.gamma * q_targets_next * (1 - dones))\n",
    "        q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        loss = nn.MSELoss()(q_expected, q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.soft_update()\n",
    "\n",
    "    def soft_update(self):\n",
    "        for target_param, local_param in zip(self.qnetwork_target.parameters(),\n",
    "                                              self.qnetwork_local.parameters()):\n",
    "            target_param.data.copy_(self.tau * local_param.data + (1.0 - self.tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640469cc-763d-4f50-a5bc-4a2ef84f3127",
   "metadata": {},
   "source": [
    "### DQN Training Loop"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a15d32c-5ba4-4327-9234-fb23cbdb5e9c",
   "metadata": {},
   "source": [
    "def train_dqn(env, agent, n_episodes = 1000, max_t = 1000,\n",
    "              eps_start = 1.0, eps_end = 0.01, eps_decay = 0.995,\n",
    "              target_score = 200.0, save_path = \"dqn_lander.pth\", verbose = True):\n",
    "    \n",
    "    scores = []  # All episode rewards\n",
    "    scores_window = deque(maxlen = 100)  # Sliding window for average reward\n",
    "    epsilon = eps_start  # Starting exploration rate\n",
    "\n",
    "    bar = trange(n_episodes, desc = \"Training\", ascii = True)  # Progress bar\n",
    "    for i_episode in bar:\n",
    "        state, _ = env.reset()  # Reset environment\n",
    "        total_reward = 0  # Episode reward counter\n",
    "\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps = epsilon)  # Select action using Îµ-greedy\n",
    "            next_state, reward, done, truncated, _ = env.step(action)  # Take step\n",
    "            agent.step(state, action, reward, next_state, done)  # Store + learn\n",
    "            state = next_state  # Move to next state\n",
    "            total_reward += reward  # Update episode reward\n",
    "\n",
    "            if done or truncated:  # Stop if episode ends\n",
    "                break\n",
    "\n",
    "        scores.append(total_reward)  # Store score\n",
    "        scores_window.append(total_reward)  # Update moving average\n",
    "\n",
    "        epsilon = max(eps_end, eps_decay * epsilon)  # Decay epsilon\n",
    "        avg_score = np.mean(scores_window)  # Average of last 100 episodes\n",
    "\n",
    "        # Show info in progress bar\n",
    "        bar.set_postfix({\n",
    "            \"Epsilon\": f\"{epsilon:.3f}\",\n",
    "            \"Score\": f\"{total_reward:.1f}\",\n",
    "            \"Avg100\": f\"{avg_score:.1f}\"\n",
    "        })\n",
    "\n",
    "        # Print every 100 episodes if enabled\n",
    "        if verbose and (i_episode + 1) % 100 == 0:\n",
    "            print(f\"\\nEpisode {i_episode + 1} | Average Score (last 100): {avg_score:.2f}\")\n",
    "\n",
    "        # Stop early if average score is good\n",
    "        if avg_score >= target_score:\n",
    "            print(f\"\\nEnvironment solved in {i_episode + 1} episodes! Avg score: {avg_score:.2f}\")\n",
    "            torch.save(agent.qnetwork_local.state_dict(), save_path)  # save model\n",
    "            break\n",
    "\n",
    "    # Save model at the end\n",
    "    if avg_score < target_score:\n",
    "        print(f\"\\n Training ended after {n_episodes} episodes. Best avg score: {avg_score:.2f}\")\n",
    "        torch.save(agent.qnetwork_local.state_dict(), save_path)\n",
    "\n",
    "    return scores  # Return list of all episode rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46925ff-e991-415a-8efd-3e414db1c412",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dbb83db0-f809-4fd2-8d62-ba74eedb63ec",
   "metadata": {},
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create the LunarLander-v3 environment\n",
    "env = gym.make(\"LunarLander-v3\")\n",
    "\n",
    "# Get the number of input features (state size)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "# Get the number of possible actions (action size)\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "# Create the DQN agent using state and action dimensions\n",
    "agent = DQNAgent(state_size = state_dim, action_size = action_dim)\n",
    "\n",
    "# Train the agent and collect episode scores\n",
    "scores = train_dqn(env, agent, n_episodes = 1000, target_score = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6d4b6e-15bb-4d8a-853a-3a78c1383f2c",
   "metadata": {},
   "source": [
    "# 4. Troubleshooting & Improvements\n",
    "\n",
    "During early experiments, the agent struggled to consistently achieve high rewards. To improve learning performance, the following steps were taken:\n",
    "\n",
    "### Key Adjustments:\n",
    "- **Training Duration:** Increased the number of episodes to allow more learning opportunities.\n",
    "- **Replay Buffer Size:** Ensured a large memory buffer (100,000+ transitions) for stable learning.\n",
    "- **Soft Updates:** Used a `tau` value of `1e-3` for gradual target network updates.\n",
    "- **Reward Monitoring:** Logged average reward across episodes and implemented early stopping.\n",
    "\n",
    "### Evaluation Data\n",
    "\n",
    "The test episode results were saved to a `.csv` and are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b420689c-3375-4aa5-a5e6-66ec8a6e6d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Episode</th>\n",
       "      <th>Reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60.249863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>222.431689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>74.346023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>â</td>\n",
       "      <td>119.009192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Episode      Reward\n",
       "0       1   60.249863\n",
       "1       2  222.431689\n",
       "2       3   74.346023\n",
       "3       â  119.009192"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and display test reward results\n",
    "df_results = pd.read_csv(\"results/dqn_test_results.csv\")\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480a8b41-2b1d-4e92-92f6-988c893d945d",
   "metadata": {},
   "source": [
    "# 5. Results\n",
    "\n",
    "### Reward Trends\n",
    "\n",
    "The following plot shows the episode rewards across training. The orange line represents the moving average over 100 episodes.\n",
    "\n",
    "![Training Reward Plot](results/dqn_training_performance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30daa60-0657-4cf0-ace9-325dff271b5b",
   "metadata": {},
   "source": [
    "# 6. Conclusion & Discussion\n",
    "\n",
    "### What Worked\n",
    "- The DQN agent successfully learned to land the LunarLander with increasing consistency.\n",
    "- Soft target updates and a replay buffer helped stabilize learning.\n",
    "- The reward curve showed clear upward trends after sufficient training time.\n",
    "\n",
    "### What Didnât Work\n",
    "- Early training showed unstable performance when:\n",
    "  - The replay buffer was too small\n",
    "  - The agent was trained for too few episodes\n",
    "  - Rendering was accidentally enabled during training (slowed down runs)\n",
    "\n",
    "### Future Improvements\n",
    "- **Reward Shaping:** Add bonuses for slowing descent or staying upright to encourage smoother landings.\n",
    "- **Longer Training:** Additional episodes could push average reward even higher and faster.\n",
    "- **Model Comparison:** Test alternative algorithms like PPO, A2C, or Dueling DQNs to see how they perform on the same task.\n",
    "- **Hyperparameter Search:** Explore different learning rates, batch sizes, or epsilon decay schedules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d4b1bb-915f-4490-b207-0b0f73335e26",
   "metadata": {},
   "source": [
    "# 7. Deliverables\n",
    "\n",
    "### Code Repository\n",
    "- GitHub URL: [Insert your GitHub repo link here]\n",
    "- The repository contains:\n",
    "  - Full DQN implementation\n",
    "  - Training and testing scripts\n",
    "  - Saved model weights\n",
    "  - Plots and evaluation results\n",
    "  - Notebook report\n",
    "\n",
    "### Demo Video / GIF\n",
    "- Demo of trained agent landing:  \n",
    "  ![Landing Demo](results/dqn_demo.gif)\n",
    "\n",
    "### References\n",
    "- [OpenAI Gymnasium: LunarLander Environment](https://www.gymlibrary.dev/environments/box2d/lunar_lander/)\n",
    "- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)\n",
    "- [TQDM Progress Bars](https://github.com/tqdm/tqdm)\n",
    "- [Matplotlib Plotting](https://matplotlib.org/stable/index.html)\n",
    "- [Pillow for Text Overlay](https://pillow.readthedocs.io/en/stable/)\n",
    "\n",
    "### Files in `results/` Folder:\n",
    "- `dqn_training_performance.png` â reward plot\n",
    "- `dqn_test_results.csv` â test episode scores\n",
    "- `dqn_demo.gif` â recorded landing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa854c1-7ba8-4a55-a870-32f94051fbff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
